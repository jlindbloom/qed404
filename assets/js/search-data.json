{
  
    
        "post0": {
            "title": "Accelerated ODE Inference with PyDEns and PyMC3",
            "content": "!!! This is a work-in-progress !!! . Traditional Approaches . As a newcommer to the PyMC3 community about a year ago, one of the things I was most excited to learn about was using PyMC3 to perform Bayesian inference for systems of ordinary differential equations (ODEs). In order to take advantage of the efficiency of the NUTS sampler you must be able to provide gradients of the model with respect to each of the model parameters, which can be tricky for ODEs. In my experiences, ignoring the case in which you happen to know the analytic solution there seems to be three approaches to achieving this: . Write an ODE solver entirely in Theano which automatically gives you ability to get gradients via autodiff. This might seem like the easiest way to get the gradients, but you lose the confidence provided by well-tested solvers and it is hard to control the errors in the calculated gradients. | Use an ODE solver coupled with the adjoint method to get the gradients by solving an associated system of ODEs. This is what is done in the package sunode. | Use an ODE solver coupled with local sensitivity analysis to get the gradients by solving an augmented system of ODEs. This is the approach taken in the PyMC3 example on the Lotka-Volterra model with manual gradients. | However, you may have noticed that sometimes these methods can be slow. Let&#39;s take a look at what might cause this. Suppose we are given some parametric ODE system $$ frac{d mathbf{x}}{dt} = f( mathbf{x}, t; theta) $$ with an initial condition $ mathbf{x}_0$ and would like to know both $ mathbf{x}(T)$ and $ frac{ partial mathbf{x}(T)}{ partial theta}$ for some later time $T &gt; 0$. Traditional methods for approximating $ mathbf{x}(T)$ boil down to computing a sequence of points $ { (t_j, mathbf{x}^{j}) }_{j=0}^{M}$ with the goal being that $ mathbf{x}^M approx mathbf{x}(T)$. We might compute this sequence using the Euler method or better yet a Runge-Kutta method. If we want to increase the accuracy of our approximation $ mathbf{x}^M approx mathbf{x}(T)$ we can decrease the timestep of our method which consequently lengthens the sequence of intermediary points we must compute. If all we care about is getting $ mathbf{x}^M$ correct and won&#39;t use the other points $ { mathbf{x}^{j} }_{j=0}^{M-1}$ for anything, we end up computing an awfully large amount of intermediary points we don&#39;t actually care about. Now you might point out that in practice we often get a whole time-series of observations and that actually we will be using many of these intermediary points. OK, fine. But what if our desired numerical accuracy of our solver requires a timestep that makes our observations sparse in time when compared to the granularity of our approximating sequence? This is effectively the same dilemma as before &mdash; we approximate the solution of the ODE at a ton of times that we don&#39;t actually care about it. Inference with ODEs using these methods can be slow since each step of MCMC must re-calculate these lengthy sequences of points for a different set of parameters $ theta$. If we want to speed up ODE inference, it seems like our time would be well-spent trying to avoid the calculation of these sequences at each step. The goal of this post is to introduce a new and exciting alternative to achieve this! . Thinking Differently About Differential Equations . A concept we need to introduce before we dive in is the flow map associated to an autonomous ODE. Suppose that we know we start at $ mathbf{x}_0 in mathbb{R}^d$ and want to know where in the phase space we&#39;ll be at time $t = tau$. The flow map $ Phi_ tau : mathbb{R}^d to mathbb{R}^d$ is the function that takes us from $ mathbb{x}_0$ directly to $ mathbb{x}( tau)$, as well as from any other possible initial condition $ mathbb{x}_0&#39;$ we could&#39;ve chosen to where it would be at time $t = tau$. In other words, $$ Phi_ tau( mathbb{x}_0) = mathbb{x}( tau) $$ where $ mathbb{x}(t)$ is the solution to the initial value problem $$ begin{align} cfrac{d mathbb{x}}{dt} &amp;= f( mathbb{x}; theta) &amp; text{for $t in [0, tau]$} mathbb{x}(0) &amp;= mathbb{x}_0 end{align} $$ If we knew the flow map $ Phi_ tau$ for any $ mathbb{x}_0 in mathbb{R}^d$ and $ tau &gt; 0$ then we wouldn&#39;t have to bother with calculating an approximating sequence with a traditional solver in order to approximate $ mathbb{x}(T)$ &mdash; we could just evaluate $ Phi_T( mathbb{x}_0) = mathbb{x}(T)$. If we also knew $ frac{ partial Phi_ tau}{ partial theta}$, the gradient of the flow map with respect to the ODE parameters, we would have all of the requisite information we need to then proceed with using a gradient-based sampler like NUTS. This is all much easier said than done, but we will proceed with trying to approximate $ Phi_ tau$ and $ frac{ partial Phi_ tau}{ partial theta}$. . Using Neural Networks to Solve ODEs . If you&#39;re reading this post and don&#39;t know much about neural networks, don&#39;t worry &mdash; I&#39;m no expert either. In this post, a &quot;neural network&quot; simply refers to some function $F_{ mathcal{ eta}} : mathbb{R}^{d_1} to mathbb{R}^{d_2}$ where $ eta$ represents some set of trainable parameters (treat these completely separate from our model parameters $ theta$). By &quot;trainable&quot; I mean that we can tweak $ eta$ in order to get $F_ eta$ to &quot;agree&quot; with some other function $G: mathbb{R}^d to mathbb{R}^d$, and by &quot;agree&quot; I mean that given some set of points $ P subset mathbb{R}^{d_1} $ we can tweak $ eta$ to get $F_ eta( mathbb{x}) approx G( mathbb{x})$ for $ mathbb{x} in P$. Recall that the flow map $ Phi_ tau$ we introduced was a function $ Phi_ tau: mathbb{R}^d to mathbb{R}^d$. Let&#39;s define a new function $ Phi$ by moving $ tau$ out of the subscript such that $ Phi( mathbb{x}_0, tau) = Phi_ tau( mathbb{x}_0)$. Now we have a function $ Phi: mathbb{R}^{d+1} to mathbb{R}^d$. Earlier we restricted our discussion to just autonomous ODEs, but what we&#39;re about to do applies to non-autonomous ODEs as well. What if we took $d_1 = d+1$, $d_2=d$, and then tried to train $F_ eta$ to agree with $ Phi$? . Enter PyDEns, a Python package for solving ODEs (and PDEs) with neural networks that does just this. I will not go into the specifics here of how to use PyDEns or the theory of what is going on under the hood since the PyDEns developers provide an excellent series of tutorial notebooks here. Perhaps it&#39;s best to start with an example of PyDEns in action. $$ cfrac{dx}{dt} = cos(t) $$ with some initial condition $x(0) = x_0$, for which we know that the analytic solution is $$ x(t) = sin(t) + x_0. $$ Ignoring the specifics of what&#39;s going on down below, let&#39;s fix $x_0 = 0$ and use PyDEns to train a neural network that solves this ODE. . import os import sys import warnings warnings.filterwarnings(&#39;ignore&#39;) from tensorflow import logging logging.set_verbosity(logging.ERROR) os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39; from tqdm import tqdm_notebook import tensorflow as tf from pydens import Solver, NumpySampler, cart_prod, add_tokens from pydens import plot_loss, plot_pair_1d, plot_2d, plot_sections_2d, plot_sections_3d import numpy as np import matplotlib.pyplot as plt add_tokens() . . ode = { &#39;n_dims&#39;: 1, &#39;form&#39;: lambda u, t : D(u, t) - cos(t), &#39;initial_condition&#39;: 0.0, } config = { &#39;pde&#39;: ode, } dg = Solver(config) sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=10.0) dg.fit(batch_size=50, sampler=sampler, n_iters=100000, bar=&quot;notebook&quot;) . That looks like it did something exciting! Let&#39;s look at the solution that our neural network predicts. . t = np.linspace(0, 10, 100) # Get the NN solution t_reshaped = t[:, None] nn_sol = dg.solve(t_reshaped)[:,0] # Get the analytic solution analytic_sol = np.sin(t) # Get the error error = np.abs(analytic_sol - nn_sol) # Plot fig, axs = plt.subplots(1, 2, figsize=(13,5)) # The solutions axs[0].plot(t, nn_sol, label=&quot;Neural Network&quot;) axs[0].plot(t, analytic_sol, label=&quot;Analytic&quot;) axs[0].set_xlabel(&quot;t&quot;) axs[0].set_ylabel(&quot;x(t)&quot;) axs[0].set_title(&quot;Solution&quot;) axs[0].legend() # The error axs[1].plot(t, error) axs[1].set_xlabel(&quot;t&quot;) axs[1].set_ylabel(&quot;Absolute Error&quot;) axs[1].set_title(&quot;Solution Error&quot;) plt.show() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T21:14:17.563913 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ That looks pretty accurate! Perhaps you&#39;re unimpressed since all we did was learn a simple sine curve, so let&#39;s try to learn to solve an entire parametric family of ODEs by adding a parameter to this ODE $$ cfrac{dx}{dt} = alpha cos(t) $$ with fixed $x_0 = 0$. The analytic solution for this ODE is just $$ x(t) = alpha sin(t) + x_0. $$ One feature of this approach is that we must specify the range of values we would like to train the parameters over. In this example, we&#39;ll train the neural network to solve the ODE for $ alpha in [-3, 3]$. . ode = { &#39;n_dims&#39;: 1, &#39;form&#39;: lambda u, t, alpha : D(u, t) - P(alpha)*cos(t), &#39;initial_condition&#39;: 0.0, } config = { &#39;pde&#39;: ode, &#39;track&#39;: {&#39;dxdalpha&#39;: lambda u, t, alpha: D(u, alpha)} } dg = Solver(config) sampler = NumpySampler(&#39;uniform&#39;, low=0.0, high=10.0) &amp; NumpySampler(&#39;uniform&#39;, low=-2.2, high=2.2) dg.fit(batch_size=50, sampler=sampler, n_iters=200000, bar=&quot;notebook&quot;) . Now let&#39;s ask our neural network to predict the solutions given several different values of $ alpha$. . t = np.linspace(0, 10, 100) alphas = [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0] nn_sols = np.zeros((len(t), len(alphas))) for j, alpha in enumerate(alphas): alpha_vec = alpha*np.ones(len(t)) inputs = np.stack([t, alpha_vec]).T nn_sol = dg.solve(inputs)[:,0] nn_sols[:,j] = nn_sol fig, axs = plt.subplots(3, 3, figsize=(13,13)) for j, ax in enumerate(fig.axes): analytic_sol = alphas[j]*np.sin(t) ax.plot(t, nn_sols[:,j], label=&quot;Neural Network&quot;) ax.plot(t, analytic_sol, label=&quot;Analytic&quot;) ax.set_title(f&quot;alpha = {alphas[j]}&quot;) ax.legend(loc=&quot;lower right&quot;) ax.set_ylim(-2.5, 2.5) plt.show() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T23:09:05.079277 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ Another feature of this approach is that we can get the gradient of the solution with respect to the parameters $ theta$ for free (virtually). We can calculate the gradient by using automatic differentiation, the same tool that is being used to train the neural network in the first place. If you&#39;re familiar with PyMC3, this is also the tool being used under-the-hood to get the gradients for use in the NUTS sampler. For any $t$ the analytic gradient of our ODE solution is $$ nabla_ theta , x(t) = cfrac{ partial x(t)}{ partial alpha} = sin(t). $$ Let&#39;s compare this to the gradients we get from querying our neural network. . t = np.linspace(0, 10, 100) alphas = [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0] nn_sols = np.zeros((len(t), len(alphas))) for j, alpha in enumerate(alphas): alpha_vec = alpha*np.ones(len(t)) inputs = np.stack([t, alpha_vec]).T nn_sol = dg.solve(inputs, fetches=&quot;dxdalpha&quot;)[:,0] nn_sols[:,j] = nn_sol fig, axs = plt.subplots(3, 3, figsize=(13,13)) for j, ax in enumerate(fig.axes): analytic_sol = np.sin(t) ax.plot(t, nn_sols[:,j], label=&quot;Neural Network&quot;) ax.plot(t, analytic_sol, label=&quot;Analytic&quot;) ax.set_title(f&quot;alpha = {alphas[j]}&quot;) ax.legend(loc=&quot;lower right&quot;) ax.set_ylim(-2.5, 2.5) plt.show() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T23:09:09.488923 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ I don&#39;t think I&#39;m going to keep this example . Given the recent pandemic, you may have heard of the SIR model for infectious diseases. The SIR model is given by $$ begin{align} cfrac{dS}{dt} &amp;= - beta I S cfrac{dI}{dt} &amp;= beta I S - gamma I cfrac{dR}{dt} &amp;= gamma I end{align} $$ where $ beta, gamma &gt; 0$. We can show that $S(t) + I(t) + R(t) = 1$, which means we actually can just look at $$ begin{align} cfrac{dS}{dt} &amp;= - beta I S cfrac{dI}{dt} &amp;= beta I S - gamma I end{align} $$ and then recover $R(t)$ as $R(t) = 1 - S(t) - I(t)$. Let&#39;s fix $$ begin{align} beta &amp;= 0.1, &amp;S(0) &amp;= 0.95, gamma &amp;= 0.05, &amp;I(0) &amp;= 0.05, end{align} $$ and use scipy.integrate.odeint to approximate the solution. . import numpy as np from scipy.integrate import odeint import matplotlib.pyplot as plt import matplotlib as mpl def sir_model(y, t, beta, gamma): S, I = y dSdt = -beta*I*S dIdt = beta*I*S - gamma*I return [dSdt, dIdt] t = np.linspace(0, 250, 1000) y0 = [0.95, 0.05] beta, gamma = 0.1, 0.05 sp_sol = odeint(sir_model, y0, t, args=(beta, gamma)) . S, I = sp_sol.T R = 1.0 - S - I fig, axs = plt.subplots(figsize=(13,5)) axs.plot(t, S, label=&quot;Susceptible&quot;) axs.plot(t, I, label=&quot;Infectious&quot;) axs.plot(t, R, label=&quot;Recovered&quot;) axs.set_xlabel(&quot;Time&quot;) axs.set_ylabel(&quot;% of Population&quot;) axs.set_title(&quot;SIR Model&quot;) axs.legend() plt.show() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T16:32:24.667719 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ Ignoring the specifics for now, let&#39;s use PyDEns to train a neural network that solves the same equation ODE as above. . import os import sys import warnings warnings.filterwarnings(&#39;ignore&#39;) from tensorflow import logging logging.set_verbosity(logging.ERROR) os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39; from tqdm import tqdm_notebook import tensorflow as tf from pydens import Solver, NumpySampler, cart_prod, add_tokens from pydens import plot_loss, plot_pair_1d, plot_2d, plot_sections_2d, plot_sections_3d add_tokens() . ode = { &#39;n_dims&#39;: 1, &#39;n_funs&#39;: 2, &#39;n_eqns&#39;: 2, &#39;form&#39;: [ lambda S, I, t: D(S, t) + beta*S*I, lambda S, I, t: D(I, t) - beta*S*I + gamma*I ], &#39;initial_condition&#39;: [[0.95],[0.05]], &#39;time_multiplier&#39;: &#39;polynomial&#39;, # &#39;bind_bc_ic&#39;: True } config = { &#39;pde&#39;: ode, #&#39;optimizer&#39;: &#39;Adam&#39; # &#39;decay&#39;: {&#39;name&#39;: &#39;cyclic&#39;, &#39;learning_rate&#39;:0.001, # &#39;max_lr&#39;: 0.01, &#39;step_size&#39;: 500}, # &#39;decay&#39;: {&#39;name&#39;: &#39;invtime&#39;, &#39;learning_rate&#39;:0.01, # &#39;decay_steps&#39;: 100, &#39;decay_rate&#39;: 0.05}, # &#39;track&#39;: {&#39;dt&#39;: lambda u, t, e: D(u, t), # &#39;d_epsilon&#39;: lambda u, t, e : D(u, e)} } #sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=30.0) dg = Solver(config) sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=250.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) . NameError Traceback (most recent call last) &lt;ipython-input-15-3eec1393ab71&gt; in &lt;module&gt; 25 #sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=30.0) 26 &gt; 27 dg = Solver(config) 28 29 c: users jonathan documents jupyter notebooks pydens pydens wrapper.py in __init__(self, config, model_class, layer_size, path) 26 model_class = model_class or config.get(&#39;model_class&#39;) or TFDeepGalerkin 27 config = self.build_config(config, layer_size, path) &gt; 28 self.model = model_class(config) 29 30 c: users jonathan documents jupyter notebooks pydens pydens batchflow batchflow models tf base.py in __init__(self, *args, **kwargs) 315 &#39;devices&#39;, &#39;leading_device&#39;, &#39;device_to_scope&#39;, &#39;scope_to_device&#39;, &#39;multi_device&#39;] 316 --&gt; 317 super().__init__(*args, **kwargs) 318 319 def store_to_attr(self, attr, graph_item, device=None): c: users jonathan documents jupyter notebooks pydens pydens batchflow batchflow models base.py in __init__(self, config, *args, **kwargs) 36 self.load(**load) 37 if build: &gt; 38 self.build(*args, **kwargs) 39 40 @property c: users jonathan documents jupyter notebooks pydens pydens batchflow batchflow models tf base.py in build(self, *args, **kwargs) 384 with tf.device(device): 385 with tf.variable_scope(self.device_to_scope[device]): --&gt; 386 self.full_config = self.combine_configs() 387 self._make_inputs(config=self.full_config[&#39;inputs&#39;], 388 data_format=self.full_config.get(&#39;common/data_format&#39;, &#39;channels_last&#39;)) c: users jonathan documents jupyter notebooks pydens pydens model_tf.py in combine_configs(self) 121 122 # Count unique usages of `P` --&gt; 123 n_parameters = get_num_parameters(form[0]) 124 125 # Convert each expression to track to list c: users jonathan documents jupyter notebooks pydens pydens syntax_tree.py in get_num_parameters(form) 75 &#34;&#34;&#34; Get number of unique parameters (created via `P` letter) in the passed form.&#34;&#34;&#34; 76 n_args = len(inspect.signature(form).parameters) &gt; 77 tree = form(*[SyntaxTreeNode(&#39;_&#39; + str(i)) for i in range(n_args)]) 78 return len(get_unique_parameters(tree)) 79 &lt;ipython-input-15-3eec1393ab71&gt; in &lt;lambda&gt;(S, I, t) 4 &#39;n_eqns&#39;: 2, 5 &#39;form&#39;: [ -&gt; 6 lambda S, I, t: D(S, t) + beta*S*I, 7 lambda S, I, t: D(I, t) - beta*S*I + gamma*I 8 ], NameError: name &#39;beta&#39; is not defined . t_reshaped = t[:, None] nn_sol = dg.solve(t_reshaped) plt.plot(t, nn_sol[:,0], label=&quot;Susceptible&quot;) plt.plot(t, nn_sol[:,1], label=&quot;Infectious&quot;) # plt.plot(t, prey, label=&quot;SP Prey&quot;) # plt.plot(t, predator, label=&quot;SP Predator&quot;) # plt.title(&quot;Predator Error&quot;) plt.legend() plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T16:33:47.643427 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ Note: Save this example for later. This is tricky because of the nullclines. . Let&#39;s take a look at the Lotka-Volterra predator prey model, since this also appears in this example PyMC3 notebook and will be useful for comparison later. The Lotka-Volterra model is given by $$ begin{align} cfrac{dx}{dt} &amp;= alpha x - beta x y cfrac{dy}{dt} &amp;= - gamma y + delta xy end{align} $$ where $ alpha, beta, gamma, delta &gt; 0$. Here $y(t)$ and $x(t)$ represent the population level of some predator and prey in some ecosystem, respectively. Let&#39;s generate some synthetic data with scipy.integrate.odeint using $x(0) = 30$ and $y(0) = 50$ as our initial conditions and fixing $$ begin{align} alpha_{ text{true}} &amp;= 0.5 beta_{ text{true}} &amp;= 0.025 gamma_{ text{true}} &amp;= 0.8 delta_{ text{true}} &amp;= 0.025 end{align} $$ . import numpy as np from scipy.integrate import odeint import matplotlib.pyplot as plt import matplotlib as mpl def lotka_volterra(y, t, alpha, beta, gamma, delta): prey, predator = y dpreydt = alpha*prey - beta*prey*predator dpredatordt = -gamma*predator + delta*prey*predator return [dpreydt, dpredatordt] t = np.linspace(0, 30, 1000) y0 = [30, 50] alpha, beta, gamma, delta = 0.5, 0.025, 0.8, 0.025 sp_sol = odeint(lotka_volterra, y0, t, args=(alpha, beta, gamma, delta)) . fig, axs = plt.subplots(1,2, figsize=(13,5)) prey, predator = sp_sol.T axs[0].plot(t, prey, label=&quot;Prey&quot;) axs[0].plot(t, predator, label=&quot;Predator&quot;) axs[0].set_xlabel(&quot;Time&quot;) axs[0].set_ylabel(&quot;Population&quot;) axs[0].set_title(&quot;Lotka-Volterra Model&quot;) axs[0].legend() axs[1].plot(prey, predator) axs[1].set_xlabel(&quot;Prey Population&quot;) axs[1].set_ylabel(&quot;Predator Population&quot;) axs[1].set_title(&quot;Lotka-Volterra Phase Space&quot;) plt.show() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T19:30:09.035483 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ Ignoring the specifics of what&#39;s going on down below for now, let&#39;s use PyDEns to train a neural network that solves the same ODE as above. . # import sys # import warnings # warnings.filterwarnings(&#39;ignore&#39;) # from tensorflow import logging # logging.set_verbosity(logging.ERROR) # os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39; # from tqdm import tqdm_notebook # import tensorflow as tf # from pydens import Solver, NumpySampler, cart_prod, add_tokens # from pydens import plot_loss, plot_pair_1d, plot_2d, plot_sections_2d, plot_sections_3d # add_tokens() # ode = { # &#39;n_dims&#39;: 1, # &#39;n_funs&#39;: 2, # &#39;n_eqns&#39;: 2, # &#39;form&#39;: [ # lambda x, y, t: D(x, t) - (30/100)*(alpha*(100*x) + beta*(100*x)*(100*y)), # lambda x, y, t: D(y, t) + (30/100)*(gamma*(100*y) - delta*(100*x)*(100*y)) # ], # &#39;initial_condition&#39;: [[30/100],[50/100]], # #&#39;time_multiplier&#39;: &#39;sigmoid&#39;, # # &#39;bind_bc_ic&#39;: True # } ode = { &#39;n_dims&#39;: 1, &#39;n_funs&#39;: 2, &#39;n_eqns&#39;: 2, &#39;form&#39;: [ lambda x, y, t: D(x, t) - alpha*x + beta*x*y, lambda x, y, t: D(y, t) + gamma*y - delta*x*y ], &#39;initial_condition&#39;: [[30],[50]], #&#39;time_multiplier&#39;: &#39;sigmoid&#39;, # &#39;bind_bc_ic&#39;: True } config = { &#39;pde&#39;: ode, #&#39;optimizer&#39;: &#39;Adam&#39; # &#39;decay&#39;: {&#39;name&#39;: &#39;cyclic&#39;, &#39;learning_rate&#39;:0.001, # &#39;max_lr&#39;: 0.01, &#39;step_size&#39;: 500}, # &#39;decay&#39;: {&#39;name&#39;: &#39;invtime&#39;, &#39;learning_rate&#39;:0.01, # &#39;decay_steps&#39;: 100, &#39;decay_rate&#39;: 0.05}, # &#39;track&#39;: {&#39;dt&#39;: lambda u, t, e: D(u, t), # &#39;d_epsilon&#39;: lambda u, t, e : D(u, e)} } #sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=30.0) dg = Solver(config) sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=10.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() # dtm = 5.0 # tms = np.arange(1.0, 30.0+dtm, dtm) # n_iter_per_tm = 10000 # for j, tm in enumerate(tms): # sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=tm) # dg.fit(batch_size=50, sampler=sampler, n_iters=(j+1)*n_iter_per_tm, bar=&#39;notebook&#39;) . &lt;matplotlib.legend.Legend at 0x1e140858c88&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T19:30:50.643472 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ plt.plot(res[:,0], res[:,1], label=&quot;NN Prey&quot;) . [&lt;matplotlib.lines.Line2D at 0x1e140a6a208&gt;] . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T19:32:29.572355 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=4.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a7f7b1f188&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:33:03.881860 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=6.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a78537cec8&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:33:20.243434 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=8.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a784a6e308&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:34:55.041440 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=8.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a7fbc765c8&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:35:23.784386 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=8.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a7fae9c2c8&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:36:33.226069 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=10.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a7f2029488&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:36:49.302126 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=10.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a785363608&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:37:54.015064 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=10.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a7f57086c8&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:40:04.405164 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ sampler = NumpySampler(&#39;uniform&#39;, dim=1, low=0.0, high=14.0) dg.fit(batch_size=50, sampler=sampler, n_iters=10000) t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a7ec3cdb48&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T16:01:59.272580 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ plt.plot(res[:,0], res[:,1], label=&quot;NN Prey&quot;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:41:24.583617 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ It looks like that did something exciting! Let&#39;s look at how our loss function evolved as the network was trained. . n_iters = len(dg.loss) it_idx = [i+1 for i in range(n_iters)] fig, axs = plt.subplots(1, 2, figsize=(13,5)) axs[0].plot(it_idx, dg.loss) axs[0].set_ylabel(&#39;Loss&#39;) axs[0].set_xlabel(&#39;Iteration&#39;) axs[1].plot(it_idx, dg.loss) axs[1].set_ylabel(&#39;Log Loss&#39;) axs[1].set_xlabel(&#39;Iteration&#39;) axs[1].set_yscale(&#39;log&#39;) plt.show() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T15:38:27.719634 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ plot_loss(dg.loss) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T13:32:05.918283 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ Now that we have our trained neural network, let&#39;s compare its estimated solution with the solution we got earlier from scipy.integrate.odeint. . t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, res[:,0], label=&quot;NN Prey&quot;) plt.plot(t, res[:,1], label=&quot;NN Predator&quot;) plt.plot(t, prey, label=&quot;SP Prey&quot;) plt.plot(t, predator, label=&quot;SP Predator&quot;) plt.title(&quot;Predator Error&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1a7f488ea88&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T13:33:21.170666 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ t_reshaped = t[:, None] res = dg.solve(t_reshaped) plt.plot(t, np.abs(prey - res[:,0]), label=&quot;Prey Error&quot;) plt.plot(t, np.abs(predator - res[:,1]), label=&quot;Predator Error&quot;) plt.title(&quot;Error&quot;) plt.legend() plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-08T13:36:07.651416 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ def check_if_stable(loss, lookback_its = ): &quot;&quot;&quot;Checks if the loss curve has flattened out. &quot;&quot;&quot; . np.mean(np.diff(dg.loss)) . -0.0008927509 . Now suppose that the observed data we see in reality is some sparsely-observed subset of this data, contaminated by some Gaussian noise with mean zero and variance $ sigma^2_{ text{true}} = 100.0$. Let&#39;s take a subset of the points from above and add on this noise. . t_idxs = np.arange(0, 1000, 50) t_obs = t[t_idxs] prey_obs, predator_obs = sol[t_idxs, :].T # Add on noise sigma_true = 10.0 prey_obs += sigma_true*np.random.randn(len(prey_obs)) predator_obs += sigma_true*np.random.randn(len(predator_obs)) . fig, axs = plt.subplots(figsize=(13,5)) axs.plot(t, prey, label=&quot;True Prey&quot;) axs.plot(t, predator, label=&quot;True Predator&quot;) axs.scatter(t_obs, prey_obs, label=&quot;Observed Prey&quot;) axs.scatter(t_obs, predator_obs, label=&quot;Observed Predator&quot;) axs.set_xlabel(&quot;Time&quot;) axs.set_ylabel(&quot;Population&quot;) axs.set_title(&quot;Lotka-Volterra Model&quot;) axs.legend() plt.show() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-07-07T21:33:03.526145 image/svg+xml Matplotlib v3.4.2, https://matplotlib.org/ An Outline of the Neural Network Approach . The &quot;New&quot; approach involves an expensive offline step that then permits ultra-fast online ODE evaluations. By offline, I mean that we can offload the computational expense associated with approximating the ODE solution independently of any data associated with our system that we will observe. This offline step will likely be more costly than if we were to just approximate the ODE solution with some traditional solver, but the point is that the cost of this offline method will allow us to query solutions of the ODE ridiculously fast when we move online to perform inference with an MCMC algorithm. . This approach will also allow us to circumvent the aforementioned burden of having to compute solutions and gradients for all intermediary time points leading up to the times of our observations. .",
            "url": "https://jlindbloom.github.io/qed404/2021/01/01/ode-inference-with-pydens-and-pymc3.html",
            "relUrl": "/2021/01/01/ode-inference-with-pydens-and-pymc3.html",
            "date": " • Jan 1, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an incoming first-year applied mathematics PhD student at Dartmouth College, originally from Dallas. I like learning about computational methods for solving (inverse) problems and quantifying uncertainty. Open-source + science is pretty neat too, so sometimes I write about this here. . . I can be reached by email at jonathan@lindbloom.com. . This website is powered by fastpages and .",
          "url": "https://jlindbloom.github.io/qed404/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jlindbloom.github.io/qed404/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}